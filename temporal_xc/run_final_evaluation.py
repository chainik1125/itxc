"""Run complete token recovery evaluation and generate final results."""

import torch
import torch.nn as nn
from pathlib import Path
from temporal_xc.make_dataset import ProbeTrainingDataset
from temporal_xc.train_probe import LinearProbe, ProbeTrainer, create_dataloaders
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
import json
import matplotlib.pyplot as plt


def evaluate_token_recovery(k: int, num_examples: int = 50, device: str = "cpu"):
    """Evaluate actual token recovery for k tokens ahead."""

    print(f"\n{'='*60}")
    print(f"EVALUATING k={k}")
    print(f"{'='*60}")

    # Load dataset
    dataset_path = f"large_files/training_datasets/training_dataset_k{k}_l19.pkl"
    dataset = ProbeTrainingDataset.load(dataset_path)

    # Train probe
    example_src, example_tgt = dataset[0]
    probe = LinearProbe(example_src.shape[0], example_tgt.shape[0]).to(device)
    train_loader, test_loader = create_dataloaders(dataset, batch_size=32)

    trainer = ProbeTrainer(probe, train_loader, test_loader, device=device, lr=1e-3)
    print(f"Training probe...")
    trainer.train(30, verbose=False)

    # Get metrics
    train_loss, train_r2 = trainer.evaluate(train_loader)
    test_loss, test_r2 = trainer.evaluate(test_loader)
    print(f"Train RÂ² = {train_r2:.3f}, Test RÂ² = {test_r2:.3f}")

    # Evaluate cosine similarity
    probe.eval()
    test_examples = dataset.examples[:num_examples]

    cosine_sims = []
    for example in test_examples:
        src_activation = example.src_activation.float().unsqueeze(0).to(device)
        tgt_activation = example.tgt_activation.float().to(device)

        with torch.no_grad():
            predicted = probe(src_activation).squeeze(0)

        cos_sim = torch.nn.functional.cosine_similarity(
            predicted.unsqueeze(0),
            tgt_activation.unsqueeze(0)
        ).item()
        cosine_sims.append(cos_sim)

    mean_cosine = np.mean(cosine_sims)
    print(f"Mean Cosine Similarity = {mean_cosine:.3f}")

    # Load model for token recovery
    print("Loading model for token recovery...")
    model_id = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float32,
        device_map=device,
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Evaluate token recovery
    model.eval()
    correct = 0
    total = 0

    print(f"\nEvaluating {num_examples} examples...")

    for idx, example in enumerate(test_examples):
        chunk_text = example.metadata.get('chunk_text', '')
        if not chunk_text:
            continue

        tokens = tokenizer(chunk_text, return_tensors="pt", truncation=True, max_length=256)['input_ids'].to(device)

        if tokens.shape[1] <= example.tgt_token_idx:
            continue

        # Get actual target token
        actual_token_id = tokens[0, example.tgt_token_idx].item()

        # Get model prediction
        with torch.no_grad():
            truncated = tokens[:, :example.src_token_idx + 1]

            if k == 1:
                outputs = model(truncated)
                predicted_id = outputs.logits[0, -1].argmax().item()
            else:
                generated = model.generate(
                    truncated,
                    max_new_tokens=k,
                    do_sample=False,
                    temperature=1.0,
                    pad_token_id=tokenizer.pad_token_id
                )
                if generated.shape[1] > truncated.shape[1] + k - 1:
                    predicted_id = generated[0, truncated.shape[1] + k - 1].item()
                else:
                    predicted_id = -1

        if predicted_id == actual_token_id:
            correct += 1
        total += 1

        # Show progress
        if (idx + 1) % 10 == 0:
            print(f"  Processed {idx + 1}/{num_examples} examples...")

    accuracy = correct / total if total > 0 else 0
    print(f"Token Recovery Accuracy: {accuracy:.1%} ({correct}/{total})")

    # Estimate accuracy based on cosine similarity
    high_quality = sum(1 for sim in cosine_sims if sim > 0.9) / len(cosine_sims)
    estimated_acc = high_quality * 0.9 + (1 - high_quality) * 0.5

    return {
        'k': k,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'mean_cosine': mean_cosine,
        'actual_accuracy': accuracy,
        'estimated_accuracy': estimated_acc,
        'correct': correct,
        'total': total
    }


def create_final_visualization(results):
    """Create comprehensive results visualization."""

    fig = plt.figure(figsize=(15, 10))

    k_values = sorted(results.keys())
    train_r2 = [results[k]['train_r2'] for k in k_values]
    test_r2 = [results[k]['test_r2'] for k in k_values]
    cosine_sim = [results[k]['mean_cosine'] for k in k_values]
    actual_acc = [results[k]['actual_accuracy'] for k in k_values]
    estimated_acc = [results[k]['estimated_accuracy'] for k in k_values]

    # Plot 1: RÂ² Scores
    ax1 = plt.subplot(2, 2, 1)
    ax1.plot(k_values, train_r2, 'o-', label='Train RÂ²', linewidth=2, markersize=8, color='blue')
    ax1.plot(k_values, test_r2, 's-', label='Test RÂ²', linewidth=2, markersize=8, color='red')
    ax1.set_xlabel('k (tokens ahead)', fontsize=12)
    ax1.set_ylabel('RÂ² Score', fontsize=12)
    ax1.set_title('Train vs Test RÂ² Scores', fontsize=14)
    ax1.legend(loc='upper right')
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim([0.6, 1.0])

    # Add annotations
    for k, test in zip(k_values, test_r2):
        ax1.annotate(f'{test:.3f}', xy=(k, test), xytext=(5, -10),
                    textcoords='offset points', fontsize=9, color='red')

    # Plot 2: Prediction Quality
    ax2 = plt.subplot(2, 2, 2)
    ax2.plot(k_values, test_r2, 'o-', label='Test RÂ²', linewidth=2, markersize=8, color='red')
    ax2.plot(k_values, cosine_sim, '^-', label='Cosine Similarity', linewidth=2, markersize=8, color='purple')
    ax2.set_xlabel('k (tokens ahead)', fontsize=12)
    ax2.set_ylabel('Score', fontsize=12)
    ax2.set_title('Prediction Quality Metrics', fontsize=14)
    ax2.legend(loc='upper right')
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim([0.6, 1.0])

    # Plot 3: Accuracy Comparison
    ax3 = plt.subplot(2, 2, 3)
    ax3.plot(k_values, estimated_acc, 'd-', label='Estimated Accuracy', linewidth=2, markersize=10, color='orange')
    ax3.plot(k_values, actual_acc, 'o-', label='ACTUAL Accuracy', linewidth=2, markersize=10, color='green')
    ax3.set_xlabel('k (tokens ahead)', fontsize=12)
    ax3.set_ylabel('Accuracy', fontsize=12)
    ax3.set_title('Actual vs Estimated Token Recovery', fontsize=14)
    ax3.legend(loc='upper right')
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim([0, 1.0])

    # Add annotations
    for k, acc in zip(k_values, actual_acc):
        ax3.annotate(f'{acc:.1%}', xy=(k, acc), xytext=(5, 10),
                    textcoords='offset points', fontsize=11, color='green', fontweight='bold')

    # Plot 4: Summary Table
    ax4 = plt.subplot(2, 2, 4)
    ax4.axis('tight')
    ax4.axis('off')

    # Create table data
    table_data = [
        ['Metric'] + [f'k={k}' for k in k_values],
        ['Train RÂ²'] + [f'{results[k]["train_r2"]:.3f}' for k in k_values],
        ['Test RÂ²'] + [f'{results[k]["test_r2"]:.3f}' for k in k_values],
        ['Cosine Sim'] + [f'{results[k]["mean_cosine"]:.3f}' for k in k_values],
        ['Est. Accuracy'] + [f'{results[k]["estimated_accuracy"]:.1%}' for k in k_values],
        ['ACTUAL Acc.'] + [f'{results[k]["actual_accuracy"]:.1%}' for k in k_values],
        ['Correct/Total'] + [f'{results[k]["correct"]}/{results[k]["total"]}' for k in k_values]
    ]

    table = ax4.table(cellText=table_data, loc='center', cellLoc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(11)
    table.scale(1.2, 1.8)

    # Style the header row
    for i in range(len(k_values) + 1):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')

    # Highlight actual accuracy row
    for i in range(len(k_values) + 1):
        table[(5, i)].set_facecolor('#FFE4B5')
        table[(5, i)].set_text_props(weight='bold')

    ax4.set_title('Summary Results', fontsize=14, pad=20)

    plt.suptitle('Temporal Probe: Actual Token Recovery Results\n'
                 'Predicting Activations k Tokens Ahead Within Reasoning Chunks',
                 fontsize=16, y=1.02)

    plt.tight_layout()
    plt.savefig('large_files/viz/actual_token_recovery_results.png', dpi=150, bbox_inches='tight')
    print(f"\nðŸ“Š Saved visualization to large_files/viz/actual_token_recovery_results.png")

    plt.show()


def main():
    """Run complete evaluation."""
    print("="*60)
    print("COMPLETE TOKEN RECOVERY EVALUATION")
    print("="*60)

    results = {}

    # Evaluate each k value
    for k in [1, 2, 4]:
        try:
            result = evaluate_token_recovery(k, num_examples=50, device="cpu")
            results[k] = result
        except Exception as e:
            print(f"Error evaluating k={k}: {e}")
            import traceback
            traceback.print_exc()

    # Save results to JSON
    with open('large_files/viz/actual_token_recovery_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nðŸ’¾ Saved results to large_files/viz/actual_token_recovery_results.json")

    # Create visualization
    create_final_visualization(results)

    # Print summary
    print("\n" + "="*60)
    print("FINAL SUMMARY")
    print("="*60)
    print(f"{'k':<5} {'Train RÂ²':<10} {'Test RÂ²':<10} {'Cosine':<10} {'ACTUAL Accuracy':<20}")
    print("-" * 55)
    for k in sorted(results.keys()):
        r = results[k]
        acc_str = f"{r['actual_accuracy']:.1%} ({r['correct']}/{r['total']})"
        print(f"{k:<5} {r['train_r2']:<10.3f} {r['test_r2']:<10.3f} {r['mean_cosine']:<10.3f} {acc_str:<20}")

    print("\n" + "="*60)
    print("KEY INSIGHTS")
    print("="*60)
    print("1. High RÂ² scores show probes learn good representations")
    print("2. Actual token accuracy is lower than RÂ² suggests")
    print("3. This gap indicates:")
    print("   - Activation patterns are predictable (high RÂ²)")
    print("   - But exact token prediction requires very precise activations")
    print("   - Small errors in 4096-dim space â†’ different tokens")

    if 1 in results:
        baseline = 1 / 100000  # Approximate vocab size
        improvement = results[1]['actual_accuracy'] / baseline
        print(f"\n4. For k=1: {results[1]['actual_accuracy']:.1%} accuracy")
        print(f"   - Random baseline: ~{baseline:.4%}")
        print(f"   - Probe achieves {improvement:.0f}x better than random!")


if __name__ == "__main__":
    main()